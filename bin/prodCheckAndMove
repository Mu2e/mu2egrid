#!/usr/bin/perl -w
#
# Checks output of "production" grid jobs (run with mu2eprodsys) and
# moves them to either an appropriate FTS dropbox, or to a "failed"
# directory.
#
# A.Gaponenko, 2015
#

use strict;
use File::Basename;
use File::Path;
use File::stat;
use Cwd 'abs_path';
use Getopt::Long;
use Digest;
use Data::Dumper;

my $faildir;
my $usefts = 1;
my $verbosity=1;
my $dryrun=0;
my $rangestr;
# the following two are set from the --dirrange option. Here are the defaults:
my $dirmin = 0;
my $dirrange = 1000;
my $dirmax  = $dirmin+$dirrange-1;
my %opt = ( faildir=>\$faildir, 'verify-data'=>1, fts =>\$usefts, verbosity=>\$verbosity, 'dry-run'=>\$dryrun, 'dirrange' => \$rangestr, help=>0 );

# constant strings used as keys in the jobStats hash
use constant GOOD => '0good'; # make it sort first in the summary output.  That string is not exposed.
# the rest of the strings are used as directory names for failed jobs
use constant NOLOG => 'nolog';
use constant LOGCHECK => 'logcheck';
use constant EXITSTAT => 'exitstat';
use constant DATASIZE => 'datasize';
use constant METAPAIRED => 'metapaired';
use constant DATACHECK => 'datacheck';
use constant DUPLICATE => 'duplicate';
my %jobStats;

# A more readable description for the summary output
my %errorCodes = (
    GOOD() => 'Good',
    NOLOG() => 'no unique log file',
    LOGCHECK() => 'corrupted log file',
    EXITSTAT() => 'bad exit status',
    DATASIZE() => 'wrong data or json file size',
    METAPAIRED() => 'a non-paired content or json file',
    DATACHECK() => 'corrupted data file',
    DUPLICATE() => 'duplicate',
    );

#----------------
# See http://mu2e.fnal.gov/atwork/computing/tapeUpload.shtml
#
my %fileFamilySuffixByTier =
    (
     cnf => 'etc',
     log => 'etc',
     sim => 'sim',
     mix => 'sim',
     dig => 'sim',
     mcs => 'sim',
     nts => 'nts',
    );

#================================================================
sub parseDirRangeOption() {
    if(defined $rangestr) {
        $rangestr =~ /^(\d+):(\d+)$/
            or die "Error: --dirrange arg should be in the form  number:number\n";
        $dirmin = $1;
        die "Error: dirmin = $dirmin is out of the 0-999 range\n" if($dirmin < 0 or $dirmin > 999);

        $dirmax = $2;
        die "Error: dirmax = $dirmax is out of the 0-999 range\n" if($dirmax < 0 or $dirmax > 999);

        die "Error: dirmin=$dirmin is greater than dirmax=$dirmax\n" unless ($dirmin <= $dirmax);

        $dirrange = $dirmax - $dirmin + 1;
    }
}

#================================================================
# selects between scratch and persistent FTS tree based on
# location of the inputs
sub ftsTopDir($) {
    my ($jobdir) = @_;
    my $inpath = abs_path($jobdir);
    my $res;

  SWITCH: {
      if ($inpath =~ m|^/pnfs/mu2e/scratch|) {
          $res = '/pnfs/mu2e/scratch/fts';
          last SWITCH;
      }
      if ($inpath =~ m|^/pnfs/mu2e/persistent|) {
          $res = '/pnfs/mu2e/persistent/fts';
          last SWITCH;
      }
    }
    die "ERROR: FTS directory for input path $inpath is not defined\n"
        unless defined $res;

    return $res;
}

#================================================================
sub getLogFileName($) {
    my ($jobdir) = @_;
    my @list = glob "$jobdir/*.log";
    return ($#list == 0) ? $list[0] : "";
}

#================================================================
sub verifyDigest($$) {
    my ($fn, $expectedDigest) = @_;
    my $dig = Digest->new('SHA-256');

    my $ret = 0;

    if(open(my $fh, '<', $fn)) {
        $dig->addfile($fh);
        if($dig->hexdigest eq $expectedDigest) {
            $ret = 1;
        }
    }

    return $ret;
}

#================================================================
sub processLogFile($) {
    my ($logFileName) = @_;

    my $payload_started = 0;
    my $payload_ok = 0;
    my $manifest_selfhash='';

    # Note: dynamic determination of hash alg would need two passes over $logFileName
    # We've always been using sha256sum on worker nodes
    my $loghash = Digest->new('SHA-256');

    my %workerFileSize;
    my %workerFileDigest;

    if(open(my $logfh, $logFileName)) {

        my $manifest_started = 0;

        while(my $line = <$logfh>) {

            if($line =~ m/^Running the command:.* mu2e /) {
                $payload_started = 1;
            }

            if($line =~ m/^mu2egrid exit status 0$/) {
                $payload_ok = 1;
            }

            if($manifest_started) {
                if($line =~ m/^#/) {
                    # extract file size information from the "ls -l" output
                    my @fields = split(/\s+/, $line);
                    if($#fields == 9) {
                        my $filename = $fields[9];
                        my $filesize = $fields[5];
                        $workerFileSize{$filename} = $filesize;
                    }
                }
                else {
                    # manifest lines not starting with '#' must list dataset files
                    if($line =~ m/^([[:xdigit:]]+)\s+([^\s]+)$/) {
                        my $digest = $1;
                        my $filename = $2;
                        $workerFileDigest{$filename} = $digest;
                    }
                    else {
                        print "\tError parsing manifest line \"$line\" in $logFileName\n" if $verbosity;
                        # returning empty list will mark this job as failed
                        return;
                    }
                }
            }

            if($line =~ m/^# mu2egrid manifest *$/) {
                $manifest_started = 1;
            }

            if($line =~ m/^# mu2egrid manifest selfcheck: ([[:xdigit:]]+) *- *$/) {
                $manifest_selfhash = $1;
            }
            else {
                $loghash->add($line);
            }
        }

        return ($manifest_selfhash, $loghash->hexdigest, \%workerFileSize, \%workerFileDigest, $payload_started, $payload_ok);
    }
    else {
        print "\tError opening log file $logFileName\n" if $verbosity;
    }
}

#================================================================
sub maybe_rename($$) {
    my ($src, $dst) = @_;
    print "\t", ($dryrun ? "Would move":"Moving" ) ,": $src ====> $dst\n" if $verbosity > 1;
    if(!$dryrun) {
        rename($src, $dst) or die "Error in rename($src, $dst): $!\n";
    }
}

#================================================================
sub moveFailedJob($$$) {
    (my $srcdir, my $dsttop, my $reason) = @_;

    ++$jobStats{$reason};

    my $subdir = "$dsttop/$reason";
    if(!-d $subdir and not $dryrun) {
        File::Path::make_path($subdir)
            or die "Error creating directory $subdir: $!\n";
    }

    maybe_rename($srcdir, $subdir .'/'. basename($srcdir));
}

#================================================================
# Use on content files, not on json!
sub ftsDropDir($$) {
    (my $srcjobdir, my $datafn) = @_;

    if($usefts) {
        # data_tier.owner.description.configuration.sequencer.file_format
        my ($tier, $owner, $description, $conf, $seq, $ext, $extra) = split(/\./, $datafn);
        die "Error parsing content file name $datafn: too many fields\n" if defined $extra;
        die "Error parsing content file name $datafn: too few fields\n" if not defined $ext;

        my $fileFamilyPrefix = ($owner eq 'mu2e') ? 'phy' : 'usr';
        die "Unknown data tier \"$tier\" for file $datafn\n" unless defined  $fileFamilySuffixByTier{$tier};
        my $fileFamily = $fileFamilyPrefix . '-' . $fileFamilySuffixByTier{$tier};

        # the fts area has subdirectories to manage large
        # numbers of files.  Pick a 3-digit subdir based on file's name
        my $dig = Digest->new('SHA-256');
        $dig->add($datafn);
#        my $twodigit = sprintf '%03d', unpack('%32L', $dig->digest) % 1000;
        my $twodigit = sprintf '%03d', $dirmin + (unpack('%32L', $dig->digest) % $dirrange);

        return join('/', (ftsTopDir($srcjobdir), $fileFamily, $twodigit ));
    }
    else {
        return "$faildir/good/" . basename($srcjobdir);
    }
}

#================================================================
# Move files to the FTS upload dropbox
sub uploadFiles($$) {
    (my $srcdir, my $filelist) = @_;

    if($usefts) {
        for my $datafn (@$filelist) {
            my $dstdir = ftsDropDir($srcdir, $datafn);
            maybe_rename("$srcdir/$datafn", "$dstdir/$datafn");
            maybe_rename("$srcdir/$datafn.json", "$dstdir/$datafn.json");
        }

        if(!$dryrun) {
            rmdir($srcdir) or warn "Can't remove $srcdir: $!";
        }
    }
    else {
        my $dstdir = ftsDropDir($srcdir, $srcdir);
        if(!$dryrun) {
            my $gooddir = dirname($dstdir);
            if(not -d $gooddir) {
                File::Path::make_path($gooddir)
                    or die "Error creating directory $gooddir: $!\n";
            }
        }
        maybe_rename($srcdir, $dstdir);
    }
}

#================================================================
sub isDuplicated($$) {
    my ($jobdir, $fn) = @_;

    # The FTS copies files from the dropbox into a permanent location,
    # and registers them in SAM.  The removal from the dropbox happens
    # only after the file is registered in SAM.  To handle the race
    # condition between this script and the FTS correctly we should
    # check the dropbox first, then check SAM.

    my $res = 0;

    my $dropname = ftsDropDir($jobdir, $fn) .'/'. basename($fn);
    $res = 1 if(-e $dropname);

    system('samweb locate-file ' . basename($fn) . ' > /dev/null 2>&1');
    my ($exit_value, $signal_num, $dumped_core)  = ($? >> 8,  $? & 127, $? & 128);

    # If shell fails to execute, $?==-1.  If bash runs but fails to find
    # the program, it returns 127.  If the program file is found but not executable
    # bash returns 126.
    if (($? == -1) or ($exit_value == 127) or ($exit_value == 126)) {
        die "samweb failed to execute (\$? = $?):  $!\n";
    }
    elsif ($signal_num) {
        die sprintf "sam_web_client died with signal $signal_num\n";
    }

    $res = 1 if($exit_value == 0); # the file is known to SAM

    if($verbosity > 1) {
        print "\tisDuplicated($jobdir, $fn) = $res\n";
    }

    return $res;
}

#================================================================
sub processJobDir($) {
    my ($jobdir) = @_;
    print "Processing $jobdir\n" if $verbosity > 1;

    my $failed = 1;
    my @filesToUpload;

  CHECK: # put all the checks into a fake loop
    # then use loop control "last" to avoid mutiple nested "if"-s.
    while(1) {

        my $logFileName = getLogFileName($jobdir);
        if(!$logFileName) {
            print "\tNo unique log file for $jobdir\n" if $verbosity;
            moveFailedJob($jobdir, $faildir, NOLOG());
            last CHECK;
        }

        # make sure the .log.json file is there.
        # this is the only check we can do on that file, because it is not
        # listed in the manifest
        my $logJsonFileName = $logFileName . ".json";
        if(not -s $logJsonFileName) {
            print "\tNo valid .log.json file in $jobdir\n" if $verbosity;
            moveFailedJob($jobdir, $faildir, DATASIZE());
            last CHECK;
        }

        my ($manifest_selfhash, $logfilehash, $workerFileSize, $workerFileDigest, $payload_started, $payload_ok)
            = processLogFile($logFileName);

        if(not defined $payload_ok) {
            moveFailedJob($jobdir, $faildir, LOGCHECK());
            last CHECK;
        }

        if($payload_started and !$payload_ok) {
            print "\tJob did not run correctly in $jobdir\n" if $verbosity;
            moveFailedJob($jobdir, $faildir, EXITSTAT());
            last CHECK;
        }

        if($manifest_selfhash ne $logfilehash) {
            print "\tLog file checksum mismatch for $jobdir\n" if $verbosity;
            moveFailedJob($jobdir, $faildir, LOGCHECK());
            last CHECK;
        }

        # Look at the files listed in the manifest:
        # Make sure each file is paired with its json
        my $jsonpattern = '\.json$';
        my @contentFiles = grep { ! /$jsonpattern/ } keys %$workerFileDigest;
        my @metaFiles = grep {  /$jsonpattern/ } keys %$workerFileDigest;
        if($#contentFiles != $#metaFiles) {
            print "\tNumber of content and json files does not agree for $jobdir\n" if $verbosity;
            moveFailedJob($jobdir, $faildir, METAPAIRED());
            last CHECK;
        }
        foreach my $f (@contentFiles) {
            my $found = grep { m"$f.json" } @metaFiles;
            if(!$found) {
                print "\tNumber of content and json files does not agree for $jobdir\n" if $verbosity;
                moveFailedJob($jobdir, $faildir, METAPAIRED());
                last CHECK;
            }
        }

        # Verify integrity of files listed in the manifest:
        foreach my $fn (keys %$workerFileDigest) {
            my $st = stat($jobdir.'/'. $fn);

            if(!$st or $st->size != $$workerFileSize{$fn}) {
                print "\tError stat-ing file $jobdir/$fn\n" if $verbosity;
                moveFailedJob($jobdir, $faildir, DATASIZE());
                last CHECK;
            }

            if($opt{'verify-data'}) {
                if(!verifyDigest($jobdir.'/'.$fn, $$workerFileDigest{$fn})) {
                    print "\tDigest mismatch for $jobdir/$fn\n" if $verbosity;
                    moveFailedJob($jobdir, $faildir, DATACHECK());
                    last CHECK;
                }
            }
        }

        #----------------
        @filesToUpload = (@contentFiles, basename($logFileName));

        # Detect duplicates
        foreach my $fn (@filesToUpload) {
            if(isDuplicated($jobdir, $fn)) {
                print "\tDuplicated file $jobdir/$fn\n" if $verbosity;
                moveFailedJob($jobdir, $faildir, DUPLICATE());
                last CHECK;
            }
        }

        #----------------
        # All checks passed
        $failed = 0;
        ++$jobStats{GOOD()};
        uploadFiles($jobdir, \@filesToUpload);
        last CHECK;
    }

    print +($failed ? "FAILED" : "OK"), ":\t$jobdir\n" if $verbosity;
}

#================================================================
sub usage() {
    my $self = basename($0);
    return <<EOF
Usage:
        $self [options] --faildir=/path/dir jobdir1 jobdir2 ...

OR (useful to avoid command line overflows)

        $self [options] --faildir=/path/dir --stdin

The supported options are

    --noverify-data     Do not compute and check a SHA-256 digest of data files.

    --topdir            An alias for --faildir, which makes more sense
                        when --no-fts is used.

    --no-fts            Move good job directories into a "good" subdir under --topdir
                        instead of placing good files into the FTS input tree.

    --dirrange=min:max  Specify a range of FTS numeric subdirectories to use.
                        The default range is $dirmin:$dirmax

    --dry-run           Analyze jobdirs, but do not move the files.

    --verbosity=<int>   Verbosity level.  The default is 1.

    --help              Print this message.

The "faildir" directory should be on the same filesystem as the
jobdirs, so that failed job outputs can be moved (not copied) into it.
EOF
;
}

#================================================================
# Process command line opts.
GetOptions(\%opt,
           "faildir|topdir=s",
           "verify-data!",
           "fts!",
           "verbosity=i",
           "dry-run",
           "dirrange=s",
           "stdin",
           "help",
           )
    or die "\nError processing command line options.\n";

if($opt{'help'}) {
    print usage();
    exit 0;
}

die "The --faildir parameter must be provided. Try the --help option.\n"
    unless defined $faildir;

die "The --stdin switch should not be used when jobdirs are given as command line arguments\n"
    if ($#ARGV >= 0 and defined $opt{'stdin'});

parseDirRangeOption();

die "No jobsidr are provided on the command line, and no --stdin switch is given. Nothing to do. Try the --help option.\n"
    if ($#ARGV < 0 and not defined $opt{'stdin'});

if(defined $opt{'stdin'}) {
    while(my $dir = <STDIN>) {
        chomp $dir;
        processJobDir($dir) if $dir;
    }
}
else {
    foreach my $dir (@ARGV) {
        processJobDir($dir);
    }
}

print "Summary: ", join(', ', (map { " $errorCodes{$_}: $jobStats{$_}" } sort keys %jobStats )), "\n";

exit 0;

#================================================================
